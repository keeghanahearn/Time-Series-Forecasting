---
title: "Toyota Camry Sales Forecasting"
author: "Keeghan Ahearn"
date: "12/10/2023"
output: html_document
---
The Data

Y: Toyota Camry Sales
The Y variable is the the US sales of Toyota Camrys. The data used is total monthly US sales figures for Toyota Camry's from January 2005 to December 2023.
https://carfigures.com/us-market-brand/toyota/camry

X1: Electric Vehicle sales
The first independent variable is the US sales of electric vehicles. The data used is total monthly US sales for electric vehicles from December 2010 to October 2023. The electric vehicles are categorized as "PEV - Plug-in Electric Vehicles", which include both Battery Electric Vehicles and Plug-in Hybrid Electric Vehicles.
https://www.anl.gov/esia/reference/light-duty-electric-drive-vehicles-monthly-sales-updates-historical-data

X2: Commuter Bus Ridership
The second independent variables is the total commuter bus ridership in the US. The data used is total monthly commuter bus ridership figures from different regions and transportation systems across the US. Ridership figures started being reported in January 2012, so the data used is January 2012 to September 2023.
https://www.transit.dot.gov/ntd/data-product/monthly-module-raw-data-release


## 2. Initial Analysis
a. Store all series in a CSV file, load the file, and convert it to a tsibble object
```{r}
# Please type your code below
library(tidyverse)
library(tsibble)
library(fpp3)
library(ggplot2)
library(tidyr)

mydata <- read.csv('real data.csv', header = TRUE, sep = ',')

totalcamrysales <- read.csv('long_join_data.csv', header = TRUE, sep = ',')

pevsales <- read.csv('Total PEV Sales.csv', header = TRUE, sep = ',')

pevsales_join <- pevsales %>%
  filter(!row_number() %in% c(1:13, 155))

merged_data <- merge(mydata, pevsales_join, by='Date')
all_merged <- merge(merged_data, totalcamrysales, by = 'Date')

all_merged$Date<-as.factor(all_merged$Date)
abis<-strptime(all_merged$Date,format="%m/%d/%Y")
all_merged$Date<-as.Date(abis,format="%Y-%m-%d")

mytimeseries <- all_merged %>%
  mutate(Month = yearmonth(Date)) %>%
  as_tsibble(index = Date)

mytimeseries <- mytimeseries %>%
  mutate(Month = yearmonth(Month)) %>%
  as_tsibble(index = Month)

```

b. Create time plots for each series (Y, X1, and X2).
```{r}

autoplot(mytimeseries, PEV.Sales)
autoplot(mytimeseries, sales)
autoplot(mytimeseries, Sum.of.Commuter.Bus.Ridership)


```

d. Take natural logarithm of each series
```{r}


lambda <- mytimeseries %>%
  features(sales, features = guerrero) %>%
  pull(lambda_guerrero)

mytimeseries %>%
  autoplot(box_cox(sales, lambda))

PEV.Sales_lambda <- mytimeseries %>%
  features(PEV.Sales, features = guerrero) %>%
  pull(lambda_guerrero)

mytimeseries %>%
  autoplot(box_cox(PEV.Sales, PEV.Sales_lambda))

Ridership_lambda <- mytimeseries %>%
  features(Sum.of.Commuter.Bus.Ridership, features = guerrero) %>%
  pull(lambda_guerrero)

mytimeseries %>%
  autoplot(box_cox(Sum.of.Commuter.Bus.Ridership, Ridership_lambda))

```

e. Decompose the Y series using the STL method with default settings, and visualize the resulting components by creating a single combined figure using `autoplot()`.
```{r}


data_dcmp <- mytimeseries %>%
  model(stl = STL(sales))

components(data_dcmp) %>%
  autoplot()

```

f. Split the data series Y into two parts, training and test data
```{r}


sales_train <- mytimeseries %>%
  filter_index('2012 Jan' ~ '2021 Apr') %>%
  select(sales)
sales_test <- mytimeseries %>%
  filter_index('2021 May' ~ .) %>%
  select(sales)

```

g. Trim series X1 and X2 to match the length of the training set in series Y. And use the trimmed series for the subsequent analysis.
```{r}

Ridership_train <- mytimeseries %>%
  filter_index('2012 Jan' ~ '2021 Apr') %>%
  select(Sum.of.Commuter.Bus.Ridership)

PEV_train <- mytimeseries %>%
  filter_index('2012 Jan' ~ '2021 Apr') %>%
  select(PEV.Sales)

```

## 3. The Various Forecasts



### 3.1 Simple Forecasting Method

a. Use various benchmark methods to forecast the test set of series Y and plot the forecasts along with training and test sets.
```{r}

sales_fit <- sales_train %>%
  model(
    Mean = MEAN(sales),
    Naive = NAIVE(sales),
    `Seasonal naive` = SNAIVE(sales),
    Drift = RW(sales ~ drift())
  )
  
sales_fc <- sales_fit %>% forecast(h = 29)

sales_fc %>%
  autoplot(sales_train, level = NULL) +
  autolayer((sales_test),
            colour = "brown") +
  labs(y = "Units Sold", title = "Forecasts for Toyota Camry") +
  guides(colour = guide_legend(title = "Forecast"))

```

b. Best method:

The seasonal naive method fit the test data the best

c. Check the residuals of seasonal naive method using `gg_tsresiduals()`.

sales_train %>% 
  model(`Seasonal naive` = SNAIVE(sales)) %>%
  gg_tsresiduals()

```

d. Use `accuracy()` to compute the forecast accuracy.
```{r}

accuracy(sales_fc, mytimeseries)

```

### 3.2 Linear Models


a. Generate a scatterplot matrix of three variables: Y, X1, and X2.
```{r}

mytimeseries %>%
  GGally::ggpairs(columns = 2:4)

```


b. Fit a linear regression with X1 and X2 to the training data, and report the results
```{r}

sales_regression <- sales_train %>%
  model(TSLM((sales) ~ (Ridership_train$Sum.of.Commuter.Bus.Ridership) + (PEV_train$PEV.Sales)))

report(sales_regression)

mytimeseries %>%
  ggplot(aes(x = (PEV.Sales), y = (sales))) +
  labs(y = 'sales',
       x = 'Plug-In Electric Vehicle Sales') +
  geom_point() +
  geom_smooth(method = 'lm', se = FALSE)

```

c. Fit a linear regression with only trend and seasonal dummies to the training data, and report the results.
```{r}

fit_sales_regression <- sales_train %>%
  model(TSLM(sales ~ trend() + season()))

report(fit_sales_regression)

```

d. Check the residuals from part c) using `gg_tsresiduals()`.
```{r}
fit_sales_regression %>%
  gg_tsresiduals()

#The residual plots shows that the residuals are centered around 0, so this would suggest that they are homoscedastic. However the ACF plot does show some expenonential trends in the beginning of the plot.

```

e. Plot the residuals against the fitted values from part c).
```{r}
augment(sales_regression) %>%
  ggplot(aes(x = .fitted, y = .resid)) +
  geom_point() + labs(x = "Fitted", y = "Residuals")
  
#The residuals do not seem evenly or randomly scattered, because they are concentrated on the right side. This would suggest heteroscedasticity.

```

f. Compute forecasts for the entire test set using the linear regression model with only trend and seasonal dummies from part c).
```{r}

fit_sales_regression %>%
  forecast() %>%
  autoplot(mytimeseries)


```

g. Create a plot that includes the forecasts from part f) along with the training and test sets.
```{r}

sales_fc %>%
  autoplot(sales_train, level = NULL) +
  autolayer((sales_test),
            colour = "brown") +
  labs(y = "Units Sold", title = "Forecasts for Toyota Camry") +
  guides(colour = guide_legend(title = "Forecast"))

```

h. Use `accuracy()` to compute the forecast accuracy.

```{r}

accuracy(sales_fc, mytimeseries)

```

### 3.3 Exponential Smoothing


a. Use `ETS()` to select an appropriate model for the training set in series Y.
```{r}

sales_ETS_fit <- sales_train %>%
  model(ETS(sales))

tidy(sales_ETS_fit)
report(sales_ETS_fit)

```

b. Run residual diagnostics.

```{r}

sales_ETS_fit %>%
  gg_tsresiduals()

```

c. Compute forecasts for the entire test set, and plot the forecasts along with the test and training sets.
```{r}

sales_ETS_fc <- sales_ETS_fit %>% forecast(h=29)


sales_ETS_fc %>%
  autoplot(sales_train, level = NULL) +
  autolayer((sales_test),
            colour = "brown") +
  labs(y = "Units Sold", title = "Forecasts for Toyota Camry") +
  guides(colour = guide_legend(title = "Forecast"))


```

d. Use `accuracy()` to measure forecast accuracy.
```{r}

accuracy(sales_ETS_fc, mytimeseries)

```

### 3.4 ARIMA Models

a. Is the training set in series Y stationary?

No, there is a trend downwards. In order for the training set to be stationary, there should be no patterns like seasonality or trend in the data, and the series should be horizontal.

b. Use differencing to obtain stationary data of the training set.
```{r}
sales_train %>%
  gg_tsdisplay(log(sales) %>%
                 difference(1), plot_type = 'partial')


```

c. Use `ARIMA()` to automatically find an appropriate seasonal ARIMA model for the training set.
```{r}
library(urca)
sales_Arima <- mytimeseries %>%
  model(
    stepwise = ARIMA(sales))

report(sales_Arima)
```


d. Fit all three seasonal ARIMA models to the training set and identify best model 
```{r}

sales_seasonal_fit <- sales_train %>%
  model(
    arima1010010 = ARIMA(sales ~ pdq(10,1,0) + PDQ(1,1,0)),
    arima0114011 = ARIMA(sales ~ pdq(0,1,14) + PDQ(0,1,1)),
    auto = ARIMA(sales, stepwise = FALSE, approx = FALSE)
  )

report(sales_seasonal_fit)

#The best model is the auto, with an AICc of 2173 because the other models are null.

```

e. Run residual diagnostics for the chosen model.
```{r}

sales_seasonal_fit %>% select(auto) %>% gg_tsresiduals(lag=12)

#The residuals appear to be white noise on the ACF plot and are centered around 0 on the histogram. The residuals are homoscedastic.

```

f. Compute forecasts from the chosen model for the entire test set and plot the forecasts along with the test and training sets.
```{r}

forecast(sales_seasonal_fit, h = 29) %>%
  filter(.model == 'auto') %>% 
           autoplot(sales_train) +
  autolayer((sales_test),
            colour = 'brown') +
  labs(y = "Units Sold", title = "Forecasts for Toyota Camry") +
  guides(colour = guide_legend(title = "Forecast"))

```

g. Use `accuracy()` to measure forecast accuracy.
```{r}

sales_seasonal_fc <- sales_seasonal_fit %>% forecast(h=29)


accuracy(sales_seasonal_fc, mytimeseries)

```


## 4 Model Comparison

a. Considering the Root Mean Squared Error (RMSE)

The "auto" model demonstrates the most accurate forecast performance. My other suggested Amira models do not work with the ARIMA parameters I set, so the auto model is the best.

```{r}
sessionInfo()
```

